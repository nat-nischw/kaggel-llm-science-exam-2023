{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers -q\n",
    "# !pip install --upgrade -q wandb\n",
    "# !pip install sentencepiece -q\n",
    "# !pip install -q accelerate -U\n",
    "# !pip install -q transformers[torch]\n",
    "# !pip install datasets colorama -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "from typing import Optional, Union\n",
    "import pandas as pd, numpy as np, torch\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG \n",
    "\n",
    "VER=2\n",
    "# TRAIN WITH SUBSET OF 60K\n",
    "NUM_TRAIN_SAMPLES = 1_024\n",
    "# PARAMETER EFFICIENT FINE TUNING\n",
    "# PEFT REQUIRES 1XP100 GPU NOT 2XT4\n",
    "USE_PEFT = False\n",
    "# NUMBER OF LAYERS TO FREEZE\n",
    "# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n",
    "FREEZE_LAYERS = 18\n",
    "# BOOLEAN TO FREEZE EMBEDDINGS\n",
    "FREEZE_EMBEDDINGS = True\n",
    "# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n",
    "MAX_INPUT = 512\n",
    "# HUGGING FACE MODEL\n",
    "# MODEL = 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli'\n",
    "MODEL = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv('./60k-data-with-context-v2/train_with_context2.csv')\n",
    "print('Validation data size:', df_valid.shape )\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./60k-data-with-context-v2/all_12_with_context2.csv')\n",
    "df_train = df_train.drop(columns=\"source\")\n",
    "df_train = df_train#.fillna('')#.sample(NUM_TRAIN_SAMPLES)\n",
    "df_train = df_train[~df_train['prompt'].isin(df_valid['prompt'])].copy()\n",
    "print('Train data size:', df_train.shape )\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
    "    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first',\n",
    "                                  max_length=MAX_INPUT, add_special_tokens=False)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "\n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "dataset_valid = Dataset.from_pandas(df_valid)\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(MODEL, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FREEZE_EMBEDDINGS:\n",
    "    print('Freezing embeddings.')\n",
    "    for param in model.deberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "if FREEZE_LAYERS>0:\n",
    "    print(f'Freezing {FREEZE_LAYERS} layers.')\n",
    "    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_3(predictions, labels):\n",
    "    map_sum = 0\n",
    "    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n",
    "    for x,y in zip(pred,labels):\n",
    "        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n",
    "        map_sum += np.sum(z)\n",
    "    return map_sum / len(predictions)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = p.predictions.tolist()\n",
    "    labels = p.label_ids.tolist()\n",
    "    return {\"map@3\": map_at_3(predictions, labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.8,\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    num_train_epochs=3,\n",
    "    report_to='none',\n",
    "    output_dir = f'./checkpoints_{VER}',\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,\n",
    "    # gradient_accumulation_steps=8,\n",
    "    # logging_steps=1200,\n",
    "    evaluation_strategy='epoch',\n",
    "    # eval_steps=1200,\n",
    "    save_strategy=\"epoch\",\n",
    "    # save_steps=1200,\n",
    "    metric_for_best_model='map@3',\n",
    "    lr_scheduler_type='cosine',\n",
    "    # weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    # load_best_model_at_end = True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_valid,\n",
    "    compute_metrics = compute_metrics,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f'./model_v{VER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, trainer\n",
    "model = AutoModelForMultipleChoice.from_pretrained(f'./model_v{VER}')\n",
    "trainer = Trainer(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/content/60k-data-with-context-v2/train_with_context2.csv')\n",
    "tokenized_test_dataset = Dataset.from_pandas(test_df).map(\n",
    "        preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_string = test_df['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n",
    "import numpy as np\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Precision at k\"\"\"\n",
    "    assert k <= len(r)\n",
    "    assert k != 0\n",
    "    return sum(int(x) for x in r[:k]) / k\n",
    "\n",
    "def MAP_at_3(predictions, true_items):\n",
    "    \"\"\"Score is mean average precision at 3\"\"\"\n",
    "    U = len(predictions)\n",
    "    map_at_3 = 0.0\n",
    "    for u in range(U):\n",
    "        user_preds = predictions[u].split()\n",
    "        user_true = true_items[u]\n",
    "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "        for k in range(min(len(user_preds), 3)):\n",
    "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "    return map_at_3 / U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n",
    "print( 'CV MAP@3 =',m )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8adbe73af6dcf57f041e980de1759080172e1f234c2c6e403df01ae2ead3fbed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
