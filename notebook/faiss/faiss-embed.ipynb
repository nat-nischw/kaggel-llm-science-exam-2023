{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "files = list(map(str, Path(\"wikipedia-20230701\").glob(\"*.parquet\")))\n",
    "files.remove('wikipedia-20230701/wiki_2023_index.parquet')\n",
    "ds = load_dataset(\"parquet\", data_files=files)\n",
    "data = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import gc\n",
    "libc = ctypes.CDLL(\"libc.so.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load form CSV\n",
    "import pandas as pd\n",
    "#data = pd.read_parquet('wikiALL.parquet')\n",
    "DOCUMENT=\"text\"\n",
    "TOPIC=\"title\"\n",
    "MAX_DATA = 9999999\n",
    "#MAX_DATA = 1000000\n",
    "\n",
    "subset_data = data.head(MAX_DATA)\n",
    "subset_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "model = FlagModel('BAAI/bge-small-en', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages: \",\n",
    "                  use_fp16=True,normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "import numpy as np\n",
    "batch_size = 620000\n",
    "for i in range(0,len(subset_data),batch_size):\n",
    "     batch = subset_data.iloc[i:i+batch_size]\n",
    "     # df_loader = DataFrameLoader(batch, page_content_column=DOCUMENT)\n",
    "     # df_document = df_loader.load()\n",
    "     # text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "     # texts = text_splitter.split_documents(df_document)\n",
    "    \n",
    "     # to_encode_text = [f\"{text.metadata['title']} {text.page_content}\" for text in texts]\n",
    "     # dataframe_text = [text.page_content for text in texts]\n",
    "    \n",
    "     # embeddings_1 = model.encode(to_encode_text,batch_size=1024)\n",
    " \n",
    "     # df = pd.DataFrame(dataframe_text, columns =['text'])\n",
    "    \n",
    "     # df.to_parquet(f'parquet/{i}.parquet')\n",
    "     # np.save(f'numpyFolder/{i}.npy', embeddings_1)\n",
    "    #  _ = gc.collect()\n",
    "    # libc.malloc_trim(0)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
