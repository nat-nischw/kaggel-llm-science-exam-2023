################## General Configs ##################
VER=11
# TRAIN WITH SUBSET OF 60K
NUM_TRAIN_SAMPLES = 1_024
# PARAMETER EFFICIENT FINE TUNING
# PEFT REQUIRES 1XP100 GPU NOT 2XT4
USE_PEFT = False
# NUMBER OF LAYERS TO FREEZE
# DEBERTA LARGE HAS TOTAL OF 24 LAYERS
FREEZE_LAYERS = 18
# BOOLEAN TO FREEZE EMBEDDINGS
FREEZE_EMBEDDINGS = True
# LENGTH OF CONTEXT PLUS QUESTION ANSWER
MAX_INPUT = 512
# HUGGING FACE MODEL
MODEL = 'microsoft/deberta-v3-large'

################## TRAINING PARAMETERS ##################
# EARLY STOPPING PATIENCE
EARLY_STOPPING = 1
# NUMBER OF EPOCHS
EPOCHS = 2
# LEARNING RATE
LR = 2e-6
# WARMUP RATIO
WARMUP_RATIO = 0.8
# BATCH SIZE
BATCH_SIZE = 4
# EVALUATION STRATEGY
EVAL_STRATEGY = 'epoch'
# SAVE STRATEGY
SAVE_STRATEGY = 'epoch'
# METRIC FOR BEST MODEL
METRIC_FOR_BEST_MODEL = 'map@3'
# LR SCHEDULER TYPE
LR_SCHEDULER_TYPE = 'cosine'
# SAVE TOTAL LIMIT
SAVE_TOTAL_LIMIT = 2
# SEED
SEED = 42
# FP16
FP16 = True
# REPORT TO
REPORT_TO = 'none'
# OVERWRITE OUTPUT DIRECTORY
OVERWRITE_OUTPUT_DIR = True

################## OUTPUT DIRECTORY ##################
# Output directory
OUTPUT_DIR = f'./output/{MODEL}-ver{VER}'

